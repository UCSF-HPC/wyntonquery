---
title: "SGE Accounting File"
author: Henrik Bengtsson
date: 2021-06-18
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SGE Accounting File}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

The SGE accounting file holds information on all submitted and _finished_ jobs submitted to the SGE scheduler.  Queued or currently running jobs are _not_ included in this file.  A job is considered "finished" if it completed successfully, terminated due to an error, or was cancelled.  The file is a colon-delimited text file with one job entry per row, with the most recently finished job appended at end.  Contrary to what one might expect, the file is not _perfectly_ ordered by the `end_time` of the jobs.  We might find some entries where the `end_time` of two consequentive entries might differ by a few seconds in the "wrong" order.  It's not clear to me why this is but it might be that the scheduler updates the SGE accounting file at regular intervals, say every few minutes, and when it does it goes through the jobs in order of job index.


## Indexing the SGE accounting file

In March 2020, it was ~12 GB and took 6-8 minutes to read in full.  As of June 2021, the Wynton HPC accounting file is ~54 GB with 160.5 million job entries.  It is rare to be interested in all entries.  It is more common to work with a subset of the job entries.  In order to do this efficiently, we start by indexing the SGE accounting file to identify the file byte offset for each job entry;

```r
library(wyntonquery)
progressr::handlers(global = TRUE) ## Report on progress
progressr::handlers("progress")

pathname <- sge_accounting_file()
cat(sprintf("File size: %.3g GB\n", file.size(pathname)/1024^3))
# File size: 54.1 GB

## It takes 5-10 minutes to index a 54 GB SGE accounting file
index <- make_file_index(pathname, skip = 4L)
cat(sprintf("Number of job entries: %d\n", length(index)))
# Number of job entries: 160835061

## Save per-job index to file
save_file_index(index, file = "accounting.index_by_row")
```

Next, we want to group the job entries by the ISO-6801 week of the job end times.

```r
library(wyntonquery)
progressr::handlers(global = TRUE) ## Report on progress
progressr::handlers("progress")

pathname <- sge_accounting_file()
index <- read_file_index("accounting.index_by_row")
week_index <- sge_make_week_index(pathname, index = index)
saveRDS(week_index, file = filename)

print(week_index)
# A tibble: 201 x 3
   week    nbr_of_jobs file_offset
   <chr>         <dbl>       <dbl>
 1 2017W33         406          59
 2 2017W34         450      113195
 3 2017W35         499      127727
 4 2017W36         523      143361
 5 2017W37         531      150740
 6 2017W38         561      153559
 7 2017W39         579      163657
 8 2017W40         601      169804
 9 2017W41         603      177075
10 2017W42         621      177695
# … with 191 more rows
```



## Reading job entries in SGE accounting file

Here is an example how we can read the job entries for a couple of weeks:

```r
library(wyntonquery)

pathname <- sge_accounting_file()
week_index <- readRDS("accounting.index_by_week.rds")

weeks <- c("2020W04", "2020W05")
offset <- week_index$file_offset[weeks[1]]
n_max <- sum(week_index$nbr_of_jobs[weeks])
jobs <- read_sge_accounting(pathname, offset = offset, n_max = n_max)
jobs <- anonymize(jobs)
print(jobs)
# A tibble: 103,419 x 45
   qname  hostname  group  owner job_name   job_number account priority submission_time     start_time          end_time            failed exit_status
   <chr>  <chr>     <chr>  <chr> <chr>           <int> <chr>      <int> <dttm>              <dttm>              <dttm>               <int>       <int>
 1 long.q qb3-id149 group… owne… ddG_satur…     273941 sge           19 2021-06-17 10:15:11 2021-06-17 16:07:29 2021-06-17 19:13:24      0           0
 2 long.q qb3-id26  group… owne… ddG_satur…     273941 sge           19 2021-06-17 10:15:11 2021-06-17 16:33:13 2021-06-17 19:13:23      0           0
 3 long.q qb3-id78  group… owne… batch_3d       275353 sge           19 2021-06-17 11:38:15 2021-06-17 18:58:08 2021-06-17 19:13:24      0           0
 4 short… qb3-id164 group… owne… min111         280589 sge           10 2021-06-17 19:09:28 2021-06-17 19:10:34 2021-06-17 19:13:25      0           0
 5 long.q qb3-id176 group… owne… ddG_satur…     273941 sge           19 2021-06-17 10:15:11 2021-06-17 16:44:48 2021-06-17 19:13:25      0           0
 6 short… qb3-id176 group… owne… min153         280621 sge           10 2021-06-17 19:09:38 2021-06-17 19:10:34 2021-06-17 19:13:25      0           0
 7 long.q qb3-ih59  group… owne… batch_3d       274703 sge           19 2021-06-17 11:28:06 2021-06-17 18:25:42 2021-06-17 19:13:25      0           0
 8 short… qb3-ih35  group… owne… min133         280751 sge           10 2021-06-17 19:10:17 2021-06-17 19:13:22 2021-06-17 19:13:26      0         132
 9 short… qb3-ih29  group… owne… min62          280708 sge           10 2021-06-17 19:10:04 2021-06-17 19:13:22 2021-06-17 19:13:26      0         132
10 short… qb3-ih29  group… owne… min69          280714 sge           10 2021-06-17 19:10:06 2021-06-17 19:13:22 2021-06-17 19:13:26      0         132
# … with 103,409 more rows, and 32 more variables: ru_wallclock <drtn>, ru_utime <drtn>, ru_stime <drtn>, ru_maxrss <chr>,
#   ru_ixrss <chr>, ru_ismrss <chr>, ru_idrss <chr>, ru_isrss <chr>, ru_minflt <dbl>, ru_majflt <dbl>, ru_nswap <dbl>, ru_inblock <dbl>,
#   ru_oublock <dbl>, ru_msgsnd <dbl>, ru_msgrcv <dbl>, ru_nsignals <dbl>, ru_nvcsw <dbl>, ru_nivcsw <dbl>, project <chr>,
#   department <chr>, granted_pe <chr>, slots <int>, task_number <int>, cpu <drtn>, mem <chr>, io <chr>, category <chr>, iow <drtn>,
#   pe_taskid <chr>, maxvmem <dbl>, arid <dbl>, ar_sub_time <dttm>
```

_Comment:_ The reason for getting more than 100,000 job entries is that the SGE accounting file had recorded another 3,419 jobs since we last indexed the file.


To read, say, 10,000 jobs starting with job entry 120,000,000, use:

```r
from <- 120000000
jobs <- read_sge_accounting(pathname, offset = index[from], n_max = 10000)
jobs <- anonymize(jobs)
print(jobs)
# A tibble: 10,000 x 45
   qname   hostname  group  owner  job_name job_number account priority submission_time     start_time          end_time            failed exit_status
   <chr>   <chr>     <chr>  <chr>  <chr>         <int> <chr>      <int> <dttm>              <dttm>              <dttm>               <int>       <int>
 1 long.q  qb3-id146 group… owner… batch_3d    2514714 sge           19 2021-03-18 05:17:48 2021-03-18 07:09:45 2021-03-18 07:40:23      0           0
 2 long.q  qb3-id66  group… owner… batch_3d    2514751 sge           19 2021-03-18 05:26:06 2021-03-18 07:20:45 2021-03-18 07:40:23      0           0
 3 member… qb3-id70  group… owner… TB_2julA    2515048 sge            0 2021-03-18 06:47:47 2021-03-18 07:38:45 2021-03-18 07:40:23      0           0
 4 long.q  qb3-ad9   group… owner… batch_3d    2514714 sge           19 2021-03-18 05:17:48 2021-03-18 07:10:45 2021-03-18 07:40:24      0           0
 5 member… qb3-id195 group… owner… TB_2julA    2515048 sge            0 2021-03-18 06:47:47 2021-03-18 07:38:45 2021-03-18 07:40:25      0           0
 6 long.q  qb3-id83  group… owner… batch_3d    2514820 sge           19 2021-03-18 05:36:24 2021-03-18 07:30:45 2021-03-18 07:40:26      0           0
 7 long.q  qb3-id31  group… owner… batch_3d    2514820 sge           19 2021-03-18 05:36:24 2021-03-18 07:29:45 2021-03-18 07:40:26      0           0
 8 long.q  qb3-id124 group… owner… batch_3d    2514827 sge           19 2021-03-18 05:38:30 2021-03-18 07:33:45 2021-03-18 07:40:26      0           0
 9 long.q  qb3-id52  group… owner… batch_3d    2514820 sge           19 2021-03-18 05:36:24 2021-03-18 07:29:45 2021-03-18 07:40:26      0           0
10 long.q  cc-id1    group… owner… batch_3d    2514783 sge           19 2021-03-18 05:30:12 2021-03-18 07:23:45 2021-03-18 07:40:27      0           0
# … with 9,990 more rows, and 32 more variables: ru_wallclock <drtn>, ru_utime <drtn>, ru_stime <drtn>, ru_maxrss <chr>, ru_ixrss <chr>,
#   ru_ismrss <chr>, ru_idrss <chr>, ru_isrss <chr>, ru_minflt <dbl>, ru_majflt <dbl>, ru_nswap <dbl>, ru_inblock <dbl>, ru_oublock <dbl>,
#   ru_msgsnd <dbl>, ru_msgrcv <dbl>, ru_nsignals <dbl>, ru_nvcsw <dbl>, ru_nivcsw <dbl>, project <chr>, department <chr>, granted_pe <chr>,
#   slots <int>, task_number <int>, cpu <drtn>, mem <chr>, io <chr>, category <chr>, iow <drtn>, pe_taskid <chr>, maxvmem <dbl>, arid <dbl>,
#   ar_sub_time <dttm>
```


## Statistics of jobs finished during the last 24 hours


```r
library(wyntonquery)

pathname <- sge_accounting_file()
index <- read_file_index("accounting.index")

## Read enough job entries to cover 24 hours and more
from <- 300e3
jobs <- read_sge_accounting(pathname, offset = index[length(index)+1-from])

period <- range(jobs$end_time, na.rm=TRUE)
cat(sprintf("Period: %s/%s\n", period[1], period[2]))
## Period: 2021-06-17 04:23:00/2021-06-18 09:26:06

## Keep jobs for the last 24 hours
jobs <- subset(jobs, end_time >= period[2] - 24*3600)
period <- range(jobs$end_time, na.rm=TRUE)
cat(sprintf("Period: %s/%s\n", period[1], period[2]))
## Period: 2021-06-15 19:35:15/2021-06-16 19:35:15
cat(sprintf("Number of jobs finished during this period: %d\n", nrow(jobs)))
## Number of jobs finished during this period: 197538

nusers <- length(unique(jobs$owner))
cat(sprintf("Number of unique users: %d\n", nusers))
# Number of unique users: 79
```

Let's see how many of the jobs finished succesfully and how many failed.

```r
## Get successful and failed jobs
groups <- list(
  success = subset(jobs, failed == 0L),
  fail    = subset(jobs, failed > 0L)
)
stats <- sapply(groups, nrow)
print(stats)
# success    fail 
#  191701    5837 

print(stats/sum(stats))
#    success       fail 
# 0.97045125 0.02954875
```

We see that failure rate among the ~200,000 jobs that ran the last 24 hours was ~3%.  Next, let's see how much CPU time this correspondsa to:

```r
## CPU time consumed
cpu <- sapply(groups, function(jobs) { d <- sum(jobs$cpu); units(d) <- "days"; d })
cat(sprintf("Total CPU processing time: %.1f hours\n", sum(cpu)))
# Total CPU processing time: 7937.3 hours

print(cpu)
#   success      fail 
# 7562.2252  375.1175

## CPU-time fractions
print(cpu / sum(cpu))
#    success       fail 
# 0.95274017 0.04725983
```

From this, we find that during the last 24 hours, ~5% of the CPU time was consumed by jobs that failed.  Among the failed jobs, the failure code was distributed as:

```r
print(table(groups$fail$failed))
#   25   26   37  100 
#    5    1 4440 1391
```

From `help("read_sge_accounting", package="wyntonquery")` for details on the parsed fields and their data types, these description of some of these codes are:

 *  25: rescheduling
 *  26: failed opening stderr/stdout file
 *  37: ran but killed because it exceeded the maximum run time (`h_rt`), maximum CPU time (`h_cpu`), or maximum virtual memory (`h_vmem`)
 * 100: ran, but killed by a signal (perhaps due to exceeding resources), task died, shepherd died (e.g. node crash), etc.


The jobs that exhausted their limits, consumed 213 days of CPU time;

```r
d <- sum(subset(groups$fail, failed == 37)$cpu); units(d) <- "days"
print(d)
# Time difference of 212.8236 days
```

which corresponds to ~3% of all CPU time spent:

```r
> as.numeric(d/sum(cpu))
[1] 0.02681296
```
